{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bloom's Taxonomy Classification Pipeline\n",
        "\n",
        "This notebook consolidates the pipeline for classifying questions into Bloom's Taxonomy levels (BT1-BT6). It includes data preparation, classical models, neural networks, transformer-based models, ensemble, zero-shot, contrastive learning, multi-task learning, explainability, and evaluation.\n",
        "\n",
        "## Requirements\n",
        "Install the following dependencies:\n",
        "```\n",
        "torch==2.0.1\n",
        "transformers==4.30.2\n",
        "scikit-learn==1.3.0\n",
        "xgboost==1.7.6\n",
        "tensorflow==2.12.0\n",
        "numpy==1.24.3\n",
        "pandas==2.0.3\n",
        "matplotlib==3.7.2\n",
        "seaborn==0.12.2\n",
        "lime==0.2.0.1\n",
        "shap==0.42.0\n",
        "captum==0.6.0\n",
        "datasets==2.13.1\n",
        "sentence-transformers==2.2.2\n",
        "huggingface-hub==0.16.4\n",
        "scipy==1.11.1\n",
        "joblib==1.3.1\n",
        "```\n",
        "Run `pip install -r requirements.txt` in your environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv (Python 3.12.10)' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '\"d:/new project/Bloom-s_Taxonomy_classification_advanced/.venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "\n",
        "# Sample CSV content (replace with full dataset)\n",
        "csv_content = \"\"\"Questions,Category\n",
        "About what proportion of the population of the US is living on farms?,BT1\n",
        "Correctly label the brain lobes indicated on the diagram below,BT1\n",
        "Define compound interest.,BT1\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(StringIO(csv_content), names=['text', 'label'], header=0)\n",
        "\n",
        "# Clean text\n",
        "df['text'] = df['text'].str.strip().str.lower().str.replace('\"', '')\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "# Split: 80/10/10 stratified\n",
        "train, test = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "val, test = train_test_split(test, test_size=0.5, stratify=test['label'], random_state=42)\n",
        "\n",
        "# Save\n",
        "os.makedirs('artifacts/data', exist_ok=True)\n",
        "train.to_csv('artifacts/data/train.csv', index=False)\n",
        "val.to_csv('artifacts/data/val.csv', index=False)\n",
        "test.to_csv('artifacts/data/test.csv', index=False)\n",
        "\n",
        "print(\"Data prepared and saved to artifacts/data/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Classical Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('artifacts/data/train.csv')\n",
        "val = pd.read_csv('artifacts/data/val.csv')\n",
        "test = pd.read_csv('artifacts/data/test.csv')\n",
        "\n",
        "# TFIDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train['text'])\n",
        "X_val = vectorizer.transform(val['text'])\n",
        "X_test = vectorizer.transform(test['text'])\n",
        "\n",
        "y_train = train['label']\n",
        "y_val = val['label']\n",
        "y_test = test['label']\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train, y_train)\n",
        "lr_pred = lr.predict(X_test)\n",
        "lr_acc = accuracy_score(y_test, lr_pred)\n",
        "lr_f1 = f1_score(y_test, lr_pred, average='weighted')\n",
        "\n",
        "# SVM\n",
        "svm = LinearSVC()\n",
        "svm.fit(X_train, y_train)\n",
        "svm_pred = svm.predict(X_test)\n",
        "svm_acc = accuracy_score(y_test, svm_pred)\n",
        "svm_f1 = f1_score(y_test, svm_pred, average='weighted')\n",
        "\n",
        "# XGBoost\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb_pred = xgb.predict(X_test)\n",
        "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
        "xgb_f1 = f1_score(y_test, xgb_pred, average='weighted')\n",
        "\n",
        "# Save models\n",
        "os.makedirs('artifacts/models', exist_ok=True)\n",
        "joblib.dump(lr, 'artifacts/models/lr.pkl')\n",
        "joblib.dump(svm, 'artifacts/models/svm.pkl')\n",
        "joblib.dump(xgb, 'artifacts/models/xgb.pkl')\n",
        "joblib.dump(vectorizer, 'artifacts/models/tfidf.pkl')\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'lr': {'acc': lr_acc, 'f1': lr_f1, 'pred': lr_pred.tolist()},\n",
        "    'svm': {'acc': svm_acc, 'f1': svm_f1, 'pred': svm_pred.tolist()},\n",
        "    'xgb': {'acc': xgb_acc, 'f1': xgb_f1, 'pred': xgb_pred.tolist()}\n",
        "}\n",
        "np.save('artifacts/results/classical_results.npy', results)\n",
        "\n",
        "print(f\"Classical models trained. LR Acc: {lr_acc:.3f}, SVM Acc: {svm_acc:.3f}, XGB Acc: {xgb_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. TextCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Custom Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab, max_len=100):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        words = text.split()\n",
        "        indices = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
        "        if len(indices) < self.max_len:\n",
        "            indices += [self.vocab['<PAD>']] * (self.max_len - len(indices))\n",
        "        else:\n",
        "            indices = indices[:self.max_len]\n",
        "        return torch.tensor(indices), torch.tensor(self.labels[idx])\n",
        "\n",
        "# TextCNN Model\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes, kernel_sizes=[3,4,5], num_filters=100):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, num_filters, (k, embed_dim)) for k in kernel_sizes\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).unsqueeze(1)\n",
        "        x = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        x = [torch.max_pool1d(xi, xi.size(2)).squeeze(2) for xi in x]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('artifacts/data/train.csv')\n",
        "val = pd.read_csv('artifacts/data/val.csv')\n",
        "test = pd.read_csv('artifacts/data/test.csv')\n",
        "\n",
        "# Build vocab\n",
        "words = ' '.join(train['text']).split()\n",
        "vocab = {word: i+2 for i, word in enumerate(set(words))}\n",
        "vocab['<PAD>'] = 0\n",
        "vocab['<UNK>'] = 1\n",
        "\n",
        "# Dataset and DataLoader\n",
        "train_dataset = TextDataset(train['text'].values, train['label'].values, vocab)\n",
        "val_dataset = TextDataset(val['text'].values, val['label'].values, vocab)\n",
        "test_dataset = TextDataset(test['text'].values, test['label'].values, vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Model, loss, optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TextCNN(len(vocab), embed_dim=100, num_classes=6).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in val_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    print(f\"Epoch {epoch+1}, Val Acc: {val_acc:.3f}\")\n",
        "\n",
        "# Test\n",
        "model.eval()\n",
        "test_preds, test_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        outputs = model(texts)\n",
        "        test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_acc = accuracy_score(test_labels, test_preds)\n",
        "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
        "\n",
        "# Save\n",
        "os.makedirs('artifacts/models', exist_ok=True)\n",
        "torch.save(model.state_dict(), 'artifacts/models/textcnn.pt')\n",
        "np.save('artifacts/results/textcnn_results.npy', {'acc': test_acc, 'f1': test_f1, 'pred': test_preds})\n",
        "\n",
        "print(f\"TextCNN trained. Test Acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab, max_len=100):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        words = text.split()\n",
        "        indices = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
        "        if len(indices) < self.max_len:\n",
        "            indices += [self.vocab['<PAD>']] * (self.max_len - len(indices))\n",
        "        else:\n",
        "            indices = indices[:self.max_len]\n",
        "        return torch.tensor(indices), torch.tensor(self.labels[idx])\n",
        "\n",
        "# BiLSTM Model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.dropout(x[:, -1, :])\n",
        "        return self.fc(x)\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('artifacts/data/train.csv')\n",
        "val = pd.read_csv('artifacts/data/val.csv')\n",
        "test = pd.read_csv('artifacts/data/test.csv')\n",
        "\n",
        "# Build vocab\n",
        "words = ' '.join(train['text']).split()\n",
        "vocab = {word: i+2 for i, word in enumerate(set(words))}\n",
        "vocab['<PAD>'] = 0\n",
        "vocab['<UNK>'] = 1\n",
        "\n",
        "# Dataset and DataLoader\n",
        "train_dataset = TextDataset(train['text'].values, train['label'].values, vocab)\n",
        "val_dataset = TextDataset(val['text'].values, val['label'].values, vocab)\n",
        "test_dataset = TextDataset(test['text'].values, test['label'].values, vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Model, loss, optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BiLSTM(len(vocab), embed_dim=100, hidden_dim=128, num_classes=6).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in val_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    print(f\"Epoch {epoch+1}, Val Acc: {val_acc:.3f}\")\n",
        "\n",
        "# Test\n",
        "model.eval()\n",
        "test_preds, test_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        outputs = model(texts)\n",
        "        test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_acc = accuracy_score(test_labels, test_preds)\n",
        "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
        "\n",
        "# Save\n",
        "os.makedirs('artifacts/models', exist_ok=True)\n",
        "torch.save(model.state_dict(), 'artifacts/models/bilstm.pt')\n",
        "np.save('artifacts/results/bilstm_results.npy', {'acc': test_acc, 'f1': test_f1, 'pred': test_preds})\n",
        "\n",
        "print(f\"BiLSTM trained. Test Acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. HAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab, max_len=100):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        words = text.split()\n",
        "        indices = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
        "        if len(indices) < self.max_len:\n",
        "            indices += [self.vocab['<PAD>']] * (self.max_len - len(indices))\n",
        "        else:\n",
        "            indices = indices[:self.max_len]\n",
        "        return torch.tensor(indices), torch.tensor(self.labels[idx])\n",
        "\n",
        "# HAN Model\n",
        "class HAN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
        "        super(HAN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.word_gru = nn.GRU(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.word_attention = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
        "        self.sentence_gru = nn.GRU(hidden_dim * 2, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.sentence_attention = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.word_gru(x)\n",
        "        word_attention = torch.tanh(self.word_attention(x))\n",
        "        word_attention = torch.softmax(word_attention, dim=1)\n",
        "        x = torch.sum(x * word_attention, dim=1)\n",
        "        x = x.unsqueeze(1)\n",
        "        x, _ = self.sentence_gru(x)\n",
        "        sentence_attention = torch.tanh(self.sentence_attention(x)).squeeze(1)\n",
        "        sentence_attention = torch.softmax(sentence_attention, dim=1)\n",
        "        x = torch.sum(x * sentence_attention.unsqueeze(2), dim=1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('artifacts/data/train.csv')\n",
        "val = pd.read_csv('artifacts/data/val.csv')\n",
        "test = pd.read_csv('artifacts/data/test.csv')\n",
        "\n",
        "# Build vocab\n",
        "words = ' '.join(train['text']).split()\n",
        "vocab = {word: i+2 for i, word in enumerate(set(words))}\n",
        "vocab['<PAD>'] = 0\n",
        "vocab['<UNK>'] = 1\n",
        "\n",
        "# Dataset and DataLoader\n",
        "train_dataset = TextDataset(train['text'].values, train['label'].values, vocab)\n",
        "val_dataset = TextDataset(val['text'].values, val['label'].values, vocab)\n",
        "test_dataset = TextDataset(test['text'].values, test['label'].values, vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Model, loss, optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = HAN(len(vocab), embed_dim=100, hidden_dim=128, num_classes=6).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in val_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    print(f\"Epoch {epoch+1}, Val Acc: {val_acc:.3f}\")\n",
        "\n",
        "# Test\n",
        "model.eval()\n",
        "test_preds, test_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        outputs = model(texts)\n",
        "        test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_acc = accuracy_score(test_labels, test_preds)\n",
        "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
        "\n",
        "# Save\n",
        "os.makedirs('artifacts/models', exist_ok=True)\n",
        "torch.save(model.state_dict(), 'artifacts/models/han.pt')\n",
        "np.save('artifacts/results/han_results.npy', {'acc': test_acc, 'f1': test_f1, 'pred': test_preds})\n",
        "\n",
        "print(f\"HAN trained. Test Acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Transformer Models (BERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import os\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('artifacts/data/train.csv')\n",
        "val = pd.read_csv('artifacts/data/val.csv')\n",
        "test = pd.read_csv('artifacts/data/test.csv')\n",
        "\n",
        "# Save as HuggingFace dataset\n",
        "train.to_csv('artifacts/data/train_hf.csv', index=False)\n",
        "val.to_csv('artifacts/data/val_hf.csv', index=False)\n",
        "test.to_csv('artifacts/data/test_hf.csv', index=False)\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset('csv', data_files={\n",
        "    'train': 'artifacts/data/train_hf.csv',\n",
        "    'validation': 'artifacts/data/val_hf.csv',\n",
        "    'test': 'artifacts/data/test_hf.csv'\n",
        "})\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
        "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='artifacts/models/bert',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy'\n",
        ")\n",
        "\n",
        "# Metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "    return {'accuracy': acc, 'f1': f1}\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Test\n",
        "test_results = trainer.predict(tokenized_datasets['test'])\n",
        "test_preds = np.argmax(test_results.predictions, axis=-1)\n",
        "test_acc = accuracy_score(test_results.label_ids, test_preds)\n",
        "test_f1 = f1_score(test_results.label_ids, test_preds, average='weighted')\n",
        "\n",
        "# Save\n",
        "os.makedirs('artifacts/models', exist_ok=True)\n",
        "model.save_pretrained('artifacts/models/bert')\n",
        "tokenizer.save_pretrained('artifacts/models/bert')\n",
        "np.save('artifacts/results/bert_results.npy', {'acc': test_acc, 'f1': test_f1, 'pred': test_preds.tolist()})\n",
        "\n",
        "print(f\"BERT trained. Test Acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import joblib\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import os\n",
        "\n",
        "# Load data\n",
        "test = pd.read_csv('artifacts/data/test.csv')\n",
        "X_test = test['text'].values\n",
        "y_test = test['label'].values\n",
        "\n",
        "# Load classical models\n",
        "vectorizer = joblib.load('artifacts/models/tfidf.pkl')\n",
        "lr = joblib.load('artifacts/models/lr.pkl')\n",
        "svm = joblib.load('artifacts/models/svm.pkl')\n",
        "xgb = joblib.load('artifacts/models/xgb.pkl')\n",
        "\n",
        "# Load transformer model\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained('artifacts/models/bert')\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained('artifacts/models/bert')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bert_model.to(device)\n",
        "bert_model.eval()\n",
        "\n",
        "# TFIDF predictions\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "lr_probs = lr.predict_proba(X_test_tfidf)\n",
        "svm_probs = svm.decision_function(X_test_tfidf)\n",
        "xgb_probs = xgb.predict_proba(X_test_tfidf)\n",
        "\n",
        "# BERT predictions\n",
        "def get_bert_probs(texts):\n",
        "    inputs = bert_tokenizer(list(texts), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = bert_model(**inputs)\n",
        "    probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "    return probs\n",
        "\n",
        "bert_probs = get_bert_probs(X_test)\n",
        "\n",
        "# Soft voting ensemble\n",
        "ensemble_probs = (lr_probs + svm_probs + xgb_probs + bert_probs) / 4\n",
        "ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
        "\n",
        "# Metrics\n",
        "ensemble_acc = accuracy_score(y_test, ensemble_preds)\n",
        "ensemble_f1 = f1_score(y_test, ensemble_preds, average='weighted')\n",
        "\n",
        "# Save\n",
        "np.save('artifacts/results/ensemble_results.npy', {'acc': ensemble_acc, 'f1': ensemble_f1, 'pred': ensemble_preds.tolist()})\n",
        "\n",
        "print(f\"Ensemble trained. Test Acc: {ensemble_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. T5 Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import os\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('artifacts/data/train.csv')\n",
        "val = pd.read_csv('artifacts/data/val.csv')\n",
        "test = pd.read_csv('artifacts/data/test.csv')\n",
        "\n",
        "# Format prompts\n",
        "def format_prompts(df):\n",
        "    df['text'] = df['text'].apply(lambda x: f\"Classify this question as BT level: {x}\")\n",
        "    df['labels'] = df['label'].apply(lambda x: f\"BT{x+1}\")\n",
        "    return df\n",
        "\n",
        "train = format_prompts(train)\n",
        "val = format_prompts(val)\n",
        "test = format_prompts(test)\n",
        "\n",
        "# Save as HuggingFace dataset\n",
        "train.to_csv('artifacts/data/train_t5.csv', index=False)\n",
        "val.to_csv('artifacts/data/val_t5.csv', index=False)\n",
        "test.to_csv('artifacts/data/test_t5.csv', index=False)\n",
        "\n",
        "dataset = load_dataset('csv', data_files={\n",
        "    'train': 'artifacts/data/train_t5.csv',\n",
        "    'validation': 'artifacts/data/val_t5.csv',\n",
        "    'test': 'artifacts/data/test_t5.csv'\n",
        "})\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    inputs = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "    labels = tokenizer(examples['labels'], padding='max_length', truncation=True, max_length=10)\n",
        "    inputs['labels'] = labels['input_ids']\n",
        "    return inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Model\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='artifacts/models/t5',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy'\n",
        ")\n",
        "\n",
        "# Metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    preds = [int(pred.replace('BT', '')) - 1 for pred in decoded_preds]\n",
        "    labels = [int(label.replace('BT', '')) - 1 for label in decoded_labels]\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    return {'accuracy': acc, 'f1': f1}\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Test\n",
        "test_results = trainer.predict(tokenized_datasets['test'])\n",
        "decoded_preds = tokenizer.batch_decode(test_results.predictions, skip_special_tokens=True)\n",
        "test_preds = [int(pred.replace('BT', '')) - 1 for pred in decoded_preds]\n",
        "test_labels = [int(label.replace('BT', '')) - 1 for label in tokenizer.batch_decode(test_results.label_ids, skip_special_tokens=True)]\n",
        "test_acc = accuracy_score(test_labels, test_preds)\n",
        "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
        "\n",
        "# Save\n",
        "os.makedirs('artifacts/models', exist_ok=True)\n",
        "model.save_pretrained('artifacts/models/t5')\n",
        "tokenizer.save_pretrained('artifacts/models/t5')\n",
        "np.save('artifacts/results/t5_results.npy', {'acc': test_acc, 'f1': test_f1, 'pred': test_preds})\n",
        "\n",
        "print(f\"T5 trained. Test Acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Zero-Shot Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import os\n",
        "\n",
        "# Load data\n",
        "test = pd.read_csv('artifacts/data/test.csv')\n",
        "X_test = test['text'].values\n",
        "y_test = test['label'].values\n",
        "\n",
        "# Zero-shot pipeline\n",
        "classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\n",
        "labels = ['BT1', 'BT2', 'BT3', 'BT4', 'BT5', 'BT6']\n",
        "\n",
        "# Predict\n",
        "test_preds = []\n",
        "for text in X_test:\n",
        "    result = classifier(text, candidate_labels=labels)\n",
        "    pred = labels.index(result['labels'][0])\n",
        "    test_preds.append(pred)\n",
        "\n",
        "# Metrics\n",
        "test_acc = accuracy_score(y_test, test_preds)\n",
        "test_f1 = f1_score(y_test, test_preds, average='weighted')\n",
        "\n",
        "# Save\n",
        "os.makedirs('artifacts/results', exist_ok=True)\n",
        "np.save('artifacts/results/zero_shot_results.npy', {'acc': test_acc, 'f1': test_f1, 'pred': test_preds})\n",
        "\n",
        "print(f\"Zero-shot trained. Test Acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Contrastive Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('artifacts/data/train.csv')\n",
        "val = pd.read_csv('artifacts/data/val.csv')\n",
        "test = pd.read_csv('artifacts/data/test.csv')\n",
        "\n",
        "# Sentence embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "X_train = model.encode(train['text'].values)\n",
        "X_val = model.encode(val['text'].values)\n",
        "X_test = model.encode(test['text'].values)\n",
        "\n",
        "y_train = train['label'].values\n",
        "y_val = val['label'].values\n",
        "y_test = test['label'].values\n",
        "\n",
        "# Logistic Regression on embeddings\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "test_preds = clf.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, test_preds)\n",
        "test_f1 = f1_score(y_test, test_preds, average='weighted')\n",
        "\n",
        "# Save\n",
        "os.makedirs('artifacts/models', exist_ok=True)\n",
        "joblib.dump(clf, 'artifacts/models/contrastive_lr.pkl')\n",
        "joblib.dump(model, 'artifacts/models/sentence_transformer.pkl')\n",
        "np.save('artifacts/results/contrastive_results.npy', {'acc': test_acc, 'f1': test_f1, 'pred': test_preds.tolist()})\n",
        "\n",
        "print(f\"Contrastive model trained. Test Acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Multi-Task Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Dataset with synthetic difficulty\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, texts, labels, difficulties, vocab, max_len=100):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.difficulties = difficulties\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        words = text.split()\n",
        "        indices = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
        "        if len(indices) < self.max_len:\n",
        "            indices += [self.vocab['<PAD>']] * (self.max_len - len(indices))\n",
        "        else:\n",
        "            indices = indices[:self.max_len]\n",
        "        return torch.tensor(indices), torch.tensor(self.labels[idx]), torch.tensor(self.difficulties[idx])\n",
        "\n",
        "# Multi-task Model\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_difficulties):\n",
        "        super(MultiTaskModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.fc_class = nn.Linear(hidden_dim * 2, num_classes)\n",
        "        self.fc_difficulty = nn.Linear(hidden_dim * 2, num_difficulties)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.dropout(x[:, -1, :])\n",
        "        class_output = self.fc_class(x)\n",
        "        difficulty_output = self.fc_difficulty(x)\n",
        "        return class_output, difficulty_output\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('artifacts/data/train.csv')\n",
        "val = pd.read_csv('artifacts/data/val.csv')\n",
        "test = pd.read_csv('artifacts/data/test.csv')\n",
        "\n",
        "# Synthetic difficulty (1-3)\n",
        "train['difficulty'] = np.random.randint(1, 4, size=len(train))\n",
        "val['difficulty'] = np.random.randint(1, 4, size=len(val))\n",
        "test['difficulty'] = np.random.randint(1, 4, size=len(test))\n",
        "\n",
        "# Build vocab\n",
        "words = ' '.join(train['text']).split()\n",
        "vocab = {word: i+2 for i, word in enumerate(set(words))}\n",
        "vocab['<PAD>'] = 0\n",
        "vocab['<UNK>'] = 1\n",
        "\n",
        "# Dataset and DataLoader\n",
        "train_dataset = MultiTaskDataset(train['text'].values, train['label'].values, train['difficulty'].values, vocab)\n",
        "val_dataset = MultiTaskDataset(val['text'].values, val['label'].values, val['difficulty'].values, vocab)\n",
        "test_dataset = MultiTaskDataset(test['text'].values, test['label'].values, test['difficulty'].values, vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Model, loss, optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MultiTaskModel(len(vocab), embed_dim=100, hidden_dim=128, num_classes=6, num_difficulties=3).to(device)\n",
        "criterion_class = nn.CrossEntropyLoss()\n",
        "criterion_difficulty = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for texts, labels, difficulties in train_loader:\n",
        "        texts, labels, difficulties = texts.to(device), labels.to(device), difficulties.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        class_output, difficulty_output = model(texts)\n",
        "        loss_class = criterion_class(class_output, labels)\n",
        "        loss_difficulty = criterion_difficulty(difficulty_output, difficulties)\n",
        "        loss = loss_class + loss_difficulty\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for texts, labels, _ in val_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            class_output, _ = model(texts)\n",
        "            val_preds.extend(torch.argmax(class_output, dim=1).cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    print(f\"Epoch {epoch+1}, Val Acc: {val_acc:.3f}\")\n",
        "\n",
        "# Test\n",
        "model.eval()\n",
        "test_preds, test_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for texts, labels, _ in test_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        class_output, _ = model(texts)\n",
        "        test_preds.extend(torch.argmax(class_output, dim=1).cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_acc = accuracy_score(test_labels, test_preds)\n",
        "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
        "\n",
        "# Save\n",
        "os.makedirs('artifacts/models', exist_ok=True)\n",
        "torch.save(model.state_dict(), 'artifacts/models/multitask.pt')\n",
        "np.save('artifacts/results/multitask_results.npy', {'acc': test_acc, 'f1': test_f1, 'pred': test_preds})\n",
        "\n",
        "print(f\"Multi-task model trained. Test Acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Explainability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from captum.attr import IntegratedGradients\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import os\n",
        "\n",
        "# Load data\n",
        "test = pd.read_csv('artifacts/data/test.csv')\n",
        "X_test = test['text'].values\n",
        "y_test = test['label'].values\n",
        "\n",
        "# Load models\n",
        "vectorizer = joblib.load('artifacts/models/tfidf.pkl')\n",
        "lr = joblib.load('artifacts/models/lr.pkl')\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained('artifacts/models/bert')\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained('artifacts/models/bert')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bert_model.to(device)\n",
        "bert_model.eval()\n",
        "\n",
        "# LIME for Logistic Regression\n",
        "explainer = LimeTextExplainer(class_names=['BT1', 'BT2', 'BT3', 'BT4', 'BT5', 'BT6'])\n",
        "def lr_predict_proba(texts):\n",
        "    return lr.predict_proba(vectorizer.transform(texts))\n",
        "\n",
        "lime_explanations = []\n",
        "for text in X_test[:5]:\n",
        "    exp = explainer.explain_instance(text, lr_predict_proba, num_features=6)\n",
        "    lime_explanations.append(exp.as_list())\n",
        "\n",
        "# Integrated Gradients for BERT\n",
        "def bert_forward(inputs):\n",
        "    attention_mask = (inputs != bert_tokenizer.pad_token_id).long()\n",
        "    outputs = bert_model(inputs, attention_mask=attention_mask)\n",
        "    return outputs.logits\n",
        "\n",
        "ig = IntegratedGradients(bert_forward)\n",
        "ig_explanations = []\n",
        "for text in X_test[:5]:\n",
        "    inputs = bert_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "    inputs = inputs['input_ids'].to(device)\n",
        "    attributions = ig.attribute(inputs, target=0)\n",
        "    ig_explanations.append(attributions.cpu().numpy())\n",
        "\n",
        "# Save\n",
        "os.makedirs('artifacts/explanations', exist_ok=True)\n",
        "np.save('artifacts/explanations/lime_lr.npy', lime_explanations)\n",
        "np.save('artifacts/explanations/ig_bert.npy', ig_explanations)\n",
        "\n",
        "print(\"Explainability analysis completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import os\n",
        "\n",
        "# Load data\n",
        "test = pd.read_csv('artifacts/data/test.csv')\n",
        "y_test = test['label'].values\n",
        "\n",
        "# Load results\n",
        "models = ['classical', 'textcnn', 'bilstm', 'han', 'bert', 'ensemble', 't5', 'zero_shot', 'contrastive', 'multitask']\n",
        "results = {}\n",
        "for model in models:\n",
        "    result = np.load(f'artifacts/results/{model}_results.npy', allow_pickle=True).item()\n",
        "    results[model] = {\n",
        "        'acc': result['acc'],\n",
        "        'f1': result['f1'],\n",
        "        'pred': result['pred'],\n",
        "        'cm': confusion_matrix(y_test, result['pred']).tolist()\n",
        "    }\n",
        "\n",
        "# Save\n",
        "os.makedirs('artifacts/evaluations', exist_ok=True)\n",
        "np.save('artifacts/evaluations/all_results.npy', results)\n",
        "\n",
        "print(\"Evaluation completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Compare Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load results\n",
        "results = np.load('artifacts/evaluations/all_results.npy', allow_pickle=True).item()\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Model': results.keys(),\n",
        "    'Accuracy': [results[m]['acc'] for m in results],\n",
        "    'Weighted F1': [results[m]['f1'] for m in results]\n",
        "})\n",
        "\n",
        "# Add macro F1\n",
        "test = pd.read_csv('artifacts/data/test.csv')\n",
        "y_test = test['label'].values\n",
        "df['Macro F1'] = [f1_score(y_test, results[m]['pred'], average='macro') for m in results]\n",
        "\n",
        "# Generate LaTeX table\n",
        "latex_table = df.to_latex(index=False, float_format=\"%.2f\", caption=\"Model Comparison on Test Set\", label=\"tab:model_comparison\")\n",
        "\n",
        "# Save LaTeX\n",
        "os.makedirs('artifacts/results', exist_ok=True)\n",
        "with open('artifacts/results/compare_results.tex', 'w') as f:\n",
        "    f.write(\"\\\\documentclass{article}\\\\usepackage{booktabs}\\\\begin{document}\\n\")\n",
        "    f.write(latex_table)\n",
        "    f.write(\"\\\\end{document}\")\n",
        "\n",
        "print(\"Comparison table generated. Run `latexmk -pdf artifacts/results/compare_results.tex` to compile.\")\n",
        "print(df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
